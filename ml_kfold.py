# -*- coding: utf-8 -*-
"""ML KFold.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jiJGbKYvROperPe7xdEcVRjzkht4E8ql

**K-Fold is a validation technique in which we divide the data into k-subsets and repeat the holdout method k times, with each of the k subsets serving as a test set and the remaining k-1 subsets serving as a training set.**

**That is, K-Fold ensures that the score of our model does not depend on the way we select our train and test subsets. In this approach, we divide the data set into k number of subsets and the holdout method is repeated k number of times.**
"""

from sklearn.linear_model import LogisticRegression 
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier 
import numpy as np
from sklearn.datasets import load_digits

"""`Digits is a collection of handwritten numbers. The intensity of one pixel in an 8 x 8 image is represented by each feature.`"""

digits = load_digits()
print(dir(digits))  # dir = directory

"""`model_selection is a technique for creating a blueprint for analyzing data and then applying it to new data. When making a prediction, choosing the right model allows us to get accurate results. To do so, we'll need to use a specific dataset to train our model. The model is then put to the test against a new dataset.`

`Sklearn model_selection has a function called train_test_split that splits data arrays into two subsets: training data and testing data. We don't have to divide the dataset manually with this function. Sklearn train_test_split creates random partitions for the two subsets by default.`
"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = 0.3)

"""*The number of items in an object is returned by the len() function. The len() function returns the number of characters in a string when the object is a string.*"""

print(len(x_train))

print(len(y_test))

"""`A Machine Learning classification algorithm called logistic regression is used to predict the probability of a categorical dependent variable. The dependent variable in logistic regression is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).`"""

lr =  LogisticRegression()
lr.fit(x_train, y_train)
lr.score(x_test, y_test)*100

"""`A Linear SVC (Support Vector Classifier) is designed to fit to the data we provide and return a "best fit" hyperplane that divides or categorizes our data. Following that, we can feed some features to our classifier to see what the "predicted" class is after we've obtained the hyperplane.`"""

sm = SVC()
sm.fit(x_train, y_train)
sm.score(x_test, y_test)*100

"""`A random forest classifier is a type of classification algorithm. A random forest is a meta estimator that uses averaging to improve predictive accuracy and control over-fitting by fitting a number of decision tree classifiers on various sub-samples of the dataset.`

"""

rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf.score(x_test, y_test)*100

"""`Model tuning and hyperparameter tuning are done using K-fold cross validation. Splitting the data into training and test data sets, applying K-fold cross-validation to the training data set, and selecting the model with the best performance is what K-fold cross validation entails.`"""

from sklearn.model_selection import KFold
kf = KFold(n_splits = 3)
kf

for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):
    print(train_index, test_index)

def get_score(model, x_train, x_test, y_train, y_test):
    model.fit(x_train, y_train)
    return model.score(x_test, y_test)

get_score(LogisticRegression(), x_train, x_test, y_train, y_test)*100

get_score(SVC(), x_train, x_test, y_train, y_test)*100

"""*Cross-validator with Stratified K-Folds. Allows us to split data into train and test sets using train/test indices. This cross-validation object returns stratified folds and is a variant of KFold. The folds are created by keeping track of the percentage of samples in each class.*"""

from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits = 3)

scores_l = []
scores_svm = []
scores_rf = []

for train_index, test_index in kf.split(digits.data):
    x_train, x_test, y_train, y_test = digits.data[train_index], digits.data[test_index], digits.target[train_index], digits.target[test_index] 

    print('LR:',get_score(LogisticRegression(),x_train, x_test, y_train, y_test)*100)
    print('SVC:',get_score(SVC(),x_train, x_test, y_train, y_test)*100)
    print('RF:',get_score(RandomForestClassifier(),x_train, x_test, y_train, y_test)*100)

    scores_l.append(get_score(LogisticRegression(),x_train, x_test, y_train, y_test)*100)
    scores_svm.append(get_score(SVC(),x_train, x_test, y_train, y_test)*100)
    scores_rf.append(get_score(RandomForestClassifier(),x_train, x_test, y_train, y_test)*100)

scores_l

scores_svm

scores_rf